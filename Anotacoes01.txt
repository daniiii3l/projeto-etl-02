INFORMAÇÕES BASE
-----------------------

# 1.0) Algumas informações relevantes:
--------------------------------------

Banco de Dados (DBeaver):
--------------------------
-> Ele é um Client do banco PostegreSQL;
-> Todo servidor é acessado por um Client;
-> O Client por sua vez é qualquer aplicação que consome esses dados do servidor;
-> Sendo assim podemos ter mais de um Client acessando o servidor;

Pontos importantes
---------------------
-> Sempre atentar-se a versão python que será utilizada para o projeto;
-> Com o ambiente virtual temos essa facilidade, pois podemos escolher a versão utilizada;


# 2.0) Resumo do projeto:
--------------------------
-> Extract: Serão obtidos do Google Drive os arquivos do tipo CSV, JSON e Parquet;
-> Transform: A transformação irá ocorrer utilizando DuckDB;
-> Load: A carga será realizada para um banco de dados PostegreSQL utilizando o Client Dbeaver;


# SETUP PROJETO:
--------------------------------
-> 1) Git: Versionamento do código;
-> 2) Pyenv: Versão do Python;
-> 3) Poetry: Ambientação virtual;



# 3.0) Etapas do projeto:
-------------------------

ETAPA 1: GitHub + Pyenv
--------------------------

-> Realizar download do GitHub, pois iremos utilizar o Git Bash;
-> Criaremos um novo repósitório dentro do GitHub;
   (GitHub) > "New Repository" > "projeto-etl-02"
-> (Git Bash) > cd Documents/Projects/projeto-etl-02
-> Após criar um novo repositório no GitHub, chamado "projeto-etl-2", iremos pegar os códigos gerados pelo GitHub,
para poder criar o arquivo ".git" na pasta do nosso projeto no diretório "Documents/Projects/projeto-etl-02" (Obs: ele ficará oculto)
	> echo "# projeto-etl-02" >> README.md
          git init
	  git add README.md
	  git commit -m "first commit"
	  git branch -M main
	  git remote add origin https://github.com/daniiii3l/projeto-etl-02.git
	  git push -u origin main

-> Podemos abrir o VScode no local em que estamos navegando pelo Git Bush (Documents/Projects/projeto-etl-02)
-> (Git Bash) > "code ."
-> (terminal code) > "pyenv local 3.13.2"

ETAPA 2: Poetry
----------------
-> O Poetry é responsavel por criar/isolar o ambiente virtual de cada projeto separadamente;
-> Poetry x Pyenv, pois o pyenv trata-se da versão do Python que iremos utilizar equanto o poetry trata-se do ambiente em si,
como por exemplo as libs que serão utilizadas individualmente para cada projeto;

-> Precisamos instalar o poetry;
	-> (Git Bash) > "curl -sSL https://install.python-poetry.org | python3 -"	
-> Criar um novo projeto:
	-> (Git Bash) > "poetry new nome-do-projeto"
-> Gerenciar o ambiente virtual:
	-> (Git Bash) > "poetry shell"
-> Para executar o script que está sendo gerenciado pelo poetry:
	-> (Git Bash) > "poetry run python nome-do-script.py"
------------------------------------------------------------------------------------------

[ INCIAIANDO ]

-> (terminal code) > "poetry init"
-> (terminal code) > "poetry shell"
-> (terminal code) > (enter) > (enter)....
-> (terminal code) > "poetry env use 3.13.2"


-> Adicionar bibliotecas utilizadas no projeto. (*** OBS: vamos instalar utilizando o poetry ao invés do pip install ****)
	
	-> (terminal code): "poetry add gdown"
	-> (terminal code): "poetry add duckdb"
	-> (terminal code): "poetry add streamlit"
	-> (terminal code): "poetry add psycopg2-binary"
	-> (terminal code): "poetry add psycopg2"
	-> (terminal code): "poetry add python-dotenv"
	-> (terminal code): "poetry add sqlalchemy"


# biblioteca "gdown": 
	-> Para baixar/utilizar o google drive
	-> Quando precisarmos utilizar algo em produção, devemos utilizar a propria biblioteca do google (google api python)
# biblioteca "psycopg2-binary": 
	-> Para trabalhar com banco de dados postegreSQL
# biblioteca "psycopg2": 
# biblioteca "streamlit":
	-> Para criação do nosso front-end
# biblioteca "duckdb":
	-> Para tratamento dos dados
# biblioteca "python-dotenv":
	-> Para conseguir vizualizar minhas variáveis
# biblioteca "sqlalchemy":
	-> Para salvar os resultados no PostegresSQL;

-> Caso queiramos gerar o arquivo de requirements.txtcom as bibliotecas instaladas:
(terminal bash) > "pip freeze >> requirements.txt


# ETAPA 3: Testar primeiro commit no github
-------------------------------------------
-> (terminal code): "git add ."
-> (terminal code): "git commit -m "build: adicionar libs"
-> (terminal code): "git push"



# ETAPA 4: Iniciando o projeto
----------------------------------
Link pasta "Financeiro" (live): https://drive.google.com/drive/folders/19flL9P8UV9aSu4iQtM6Ymv-77VtFcECP
Link pasta "Financeiro" (minha): 'https://drive.google.com/drive/folders/1Inda8n1yz05fnVzcRBtNOWGMxABQR5Wh?'


# 1. EXTRAÇÃO
------------
-> Scrit para baixar os arquivos do Google Drive;
	- Para baixar os arquivos, devemos criar uma pasta dentro do Drive;
	- Tornar a pasta Pública;
	- Obter o link de compartilhamento para fornecer no script (Obs: Retirar a parte final do link - pós interrogação)


---------------------------------------------------------//---------------------------------------------------------------------------------

# EXTRA (MEIO DO CAMINHO) - Criação de banco PostegreSQL 
--------------------------------------------------------------

-> Criaremos um banco em PostegreSQL em um Servidor Free (Semelhante a AWS);
-> Utilizaremos para isso o "Render"
-> (Render) > "+New" > "Postegre"
	-> Informações para o banco que criamos;
		-> Name: workshop-ao-vivo
		-> Database: dbname;
		-> User: dbuser
		-> (versão Free)
-> Iremos acessar o banco criado com Dbeaver (Client);
-> Criaremos em nova conexão ("+") -> (Nova Conexão) > (PostegreSQL)
-> Agora iremos copiar as informaçõs do nosso banco gerada pela Render:
	- Host: dpg-cupri93tq21c739vsf90-a.oregon-postgres.render.com (*** OBS ***: copiar link do host gerado na render: (tudo que vier após o "@" até render.com))
	- Banco de dados: (copiar nome do banco gerado pela Render)
	- User: (copiar nome do user gerado pela Render)
	- Password: (copiar nome do password gerado pela Render)
-> (Testar Conexão)

-> Criando as tabelas no Dbeaver:
-> (Selecionar banco após conexão) > (Esquemas) > (public) > (Gerar novo SQL) > (Novo Script)

---------------------------------------- // ----------------------------------
CREATE TABLE vendas (
venda_id SERIAL PRIMARY KEY,
data_venda DATE NOT NULL,
valor DECIMAL (10, 2) NOT NULL,
quantidade INT NOT NULL,
cliente_id INT NOT NULL,
categoria VARCHAR(255) NOT NULL
);



CREATE TABLE vendas_calculado (
venda_id SERIAL PRIMARY KEY,
data_venda DATE NOT NULL,
valor DECIMAL (10, 2) NOT NULL,
quantidade INT NOT NULL,
cliente_id INT NOT NULL,
categoria VARCHAR(255) NOT NULL,
total_vendas DECIMAL(10, 2) NOT NULL
);


insert into vendas (data_venda, valor, quantidade, cliente_id, categoria) values
('2024-01-01', 66.1, 10, 1, 'Alimentos'),
('2024-01-01', 50.33, 5, 2, 'Brinquedos'),
('2024-01-01', 40.2, 14, 1, 'Alimentos'),
('2024-01-02', 70.22, 10, 2, 'Brinquedos'),
('2024-01-02', 105.0, 6, 3, 'Eletronicos'),
('2024-01-02', 243.22, 7, 3, 'Eletronicos'),
('2024-01-03', 60.1, 10, 1, 'Alimentos'),
('2024-01-03', 500.54, 10, 1, 'Alimentos'),
('2024-01-03', 33.0, 20, 2, 'Brinquedos'),
('2024-01-03', 25.0, 4, 1, 'Alimentos'),
('2024-01-03', 33.1, 10, 3, 'Eletronicos'),
('2024-01-03', 54.1, 10, 2, 'Brinquedos'),
('2024-01-04', 78.2, 10, 1, 'Alimentos'),
('2024-01-04', 77.1, 14, 2, 'Brinquedos'),
('2024-01-04', 80.15, 3, 1, 'Eletronicos'),
('2024-01-04', 66.1, 10, 2, 'Brinquedos'),
('2024-01-05', 677.1, 1, 3, 'Eletronicos'),
('2024-01-05', 42.1, 22, 1, 'Alimentos'),
('2024-01-05', 330.1, 2, 1, 'Alimentos');



insert into vendas_calculado (data_venda, valor, quantidade, cliente_id, categoria, total_vendas)
select data_venda, valor, quantidade, cliente_id, categoria, (valor * quantidade) as total_vendas
from vendas;


---------------------------------------- // ----------------------------------------------------------------------------

# ETAPA 5: Salvar bases baixadas do Drive em uma tabela PostegreSQL
-------------------------------------------------------------------------

-> Apos criadas as tabelas no Dbeaver no qual iremos inserir os dados;
-> Iremos criar uma função python que recebe o DataFrame que será salvo e o nome da tabela que irá salvar os dados;
-> Pontos importantes:
	-> Fornecer a URL externa do nosso DATABASE criado no Render;
	-> Copiar o link fornecido pelo site do Render (Lembrar de colocar o número da porta pós o ".com")
	-> Criaremos um arquivo no vscode do tipo ".env" para receber a variavel com a URL da nossa DATABASE;
	-> (VSCode) > "new_file" > ".env" > DATABASE_URL = postgresql://dbuser:pUGvR5s52BQbCjHy8Al615xpN3TBPwDj@dpg-cupri93tq21c739vsf90-a.oregon-postgres.render.com:5432/dbname_erfc
	-> *** OBS ***: Precisamos carregar as variáveis de ambiente do ".env" (Inserir o "load_dotenv()" no script python após as importações)


---------------------------------------- // ----------------------------------------------------------------------------

# ETAPA 6: Refinando o processo
---------------------------------

-> Nessa etapa podemos utilizar a propria biblioteca "duckdb" para garantir que os dados os arquivos que serão baixados e inseridos no banco
não sejam sempre os mesmos;
-> A partir da biblioteca "duckdb" iremos criar um arquivo de banco de dados com o nome "duckdb.db";
-> Após criar um arquivo de banco de dados, iremos realizar a criação de uma tabela dentro desse arquivo de banco;
-> Após a criação da tabela iremos realizar o insert com o nome do arquivo e a data do arquivo processado;
-> A cada etapa em que o arquivo lido ele irá realizar o registro do arquivo nessa tabela;
-> Por fim temos uma função que irá retornar um select na tabela duckdb com os arquivos em um formato de set de dados;
-> Iremos realizar um check para identificar se o arquivo baixado já foi registrado na tabela duckdb antes de realizar a carga para a tabela de vendas do banco PostegreSQL;